{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dimitriroulin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dimitriroulin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dimitriroulin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation complete. Augmented dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms of a word.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, lang='fra'):\n",
    "        for lemma in syn.lemmas(lang='fra'):\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n=2):\n",
    "    \"\"\"Replace n words in the sentence with their synonyms.\"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:  # Only replace up to n words\n",
    "            break\n",
    "\n",
    "    # Reconstruct the sentence\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "# Load your dataset\n",
    "file_path = './training_data.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Sample 2400 sentences for augmentation\n",
    "augmentation_sample = data.sample(n=2400, random_state=2)\n",
    "augmentation_sample['sentence'] = augmentation_sample['sentence'].apply(synonym_replacement)\n",
    "\n",
    "# Combine the original and augmented data\n",
    "augmented_dataset = pd.concat([data, augmentation_sample]).reset_index(drop=True)\n",
    "\n",
    "# Save the augmented dataset to a new CSV file\n",
    "augmented_dataset.to_csv('Data-Augmentation/augmented_training_data.csv', index=False)\n",
    "\n",
    "print(\"Data augmentation complete. Augmented dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation complete. Augmented dataset saved.\n",
      "New sentences saved to new_sentences.csv.\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "file_path = 'training_data.csv'  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Get the last ID of the original sentences\n",
    "last_id = data['id'].max()\n",
    "\n",
    "# Sample sentences for augmentation (1200 sentences)\n",
    "augmentation_sample = data.sample(n=1200, random_state=1)\n",
    "augmentation_sample['sentence'] = augmentation_sample['sentence'].apply(synonym_replacement)\n",
    "\n",
    "# Increment the ID of the new sentences\n",
    "augmentation_sample['id'] = augmentation_sample['id'].apply(lambda x: x + last_id + 1)\n",
    "\n",
    "# Combine the original and augmented data\n",
    "augmented_dataset = pd.concat([data, augmentation_sample]).reset_index(drop=True)\n",
    "\n",
    "# Save the augmented dataset to a new CSV file\n",
    "augmented_dataset.to_csv('Data-Augmentation/augmented_training_data.csv', index=False)\n",
    "\n",
    "# Create a new CSV file containing only the new sentences\n",
    "new_sentences = augmentation_sample[['id', 'sentence']]\n",
    "new_sentences.to_csv('Data-Augmentation/new_sentences.csv', index=False)\n",
    "\n",
    "print(\"Data augmentation complete. Augmented dataset saved.\")\n",
    "print(\"New sentences saved to new_sentences.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combination complete. Combined dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first file\n",
    "df1 = pd.read_csv('./combined_data.csv')\n",
    "\n",
    "# Read the second file\n",
    "df2 = pd.read_csv('./Data-Augmentation/gpt_augmented_test_1.csv')\n",
    "\n",
    "# Rename the columns of df2\n",
    "df2 = df2.rename(columns={'sentence': 'sentence', 'dfficulty': 'difficulty'})\n",
    "\n",
    "# Check if 'id' column exists in df2\n",
    "if 'id' not in df2.columns:\n",
    "    # If 'id' column does not exist, create a new column with incremental values\n",
    "    df2['id'] = range(len(df2)) + df1['id'].max() + 1\n",
    "\n",
    "# Concatenate df1 and df2\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('combined_data.csv', index=False)\n",
    "\n",
    "print(\"Data combination complete. Combined dataset saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
