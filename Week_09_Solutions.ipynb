{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg25MkDgcADy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Week_9/Week_09_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLbG_J_EdcyH",
        "outputId": "286f13a8-316f-4ffc-8ba5-556368e6d57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.11/site-packages (0.0.335)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.0.64)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (1.26.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.4.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /opt/homebrew/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: wikipedia in /opt/homebrew/lib/python3.11/site-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Requirement already satisfied: chromadb in /opt/homebrew/lib/python3.11/site-packages (0.4.17)\n",
            "Requirement already satisfied: requests>=2.28 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (2.4.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.104.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0.post1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (4.8.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.16.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.14.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.59.2)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (28.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.26.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /opt/homebrew/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/homebrew/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.23.4)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3<2.0,>=1.24.2 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
            "Requirement already satisfied: coloredlogs in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.0)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.42b0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.17.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/lib/python3.11/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
            "Requirement already satisfied: tiktoken in /opt/homebrew/lib/python3.11/site-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: pypdf in /opt/homebrew/lib/python3.11/site-packages (3.17.1)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss-gpu-1.7.1.post2.tar.gz (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
            "  Downloading faiss-gpu-1.7.1.post1.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
            "  Downloading faiss-gpu-1.7.1.tar.gz (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
            "  Downloading faiss-gpu-1.7.0.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
            "  Downloading faiss-gpu-1.6.5.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
            "  Downloading faiss-gpu-1.6.4.post2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
            "  Downloading faiss-gpu-1.6.4.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m /opt/homebrew/lib/python3.11/site-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
            "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
            "  \u001b[31m   \u001b[0m running egg_info\n",
            "  \u001b[31m   \u001b[0m creating /private/var/folders/wb/btfl8bv91b5c253kbpftk1940000gn/T/pip-pip-egg-info-3b4z8x5_/faiss_cpu.egg-info\n",
            "  \u001b[31m   \u001b[0m writing /private/var/folders/wb/btfl8bv91b5c253kbpftk1940000gn/T/pip-pip-egg-info-3b4z8x5_/faiss_cpu.egg-info/PKG-INFO\n",
            "  \u001b[31m   \u001b[0m writing dependency_links to /private/var/folders/wb/btfl8bv91b5c253kbpftk1940000gn/T/pip-pip-egg-info-3b4z8x5_/faiss_cpu.egg-info/dependency_links.txt\n",
            "  \u001b[31m   \u001b[0m writing top-level names to /private/var/folders/wb/btfl8bv91b5c253kbpftk1940000gn/T/pip-pip-egg-info-3b4z8x5_/faiss_cpu.egg-info/top_level.txt\n",
            "  \u001b[31m   \u001b[0m writing manifest file '/private/var/folders/wb/btfl8bv91b5c253kbpftk1940000gn/T/pip-pip-egg-info-3b4z8x5_/faiss_cpu.egg-info/SOURCES.txt'\n",
            "  \u001b[31m   \u001b[0m error: package directory '/private/var/folders/wb/btfl8bv91b5c253kbpftk1940000gn/T/pip-install-mcrqgzrk/faiss-gpu_5a3c6b15ca114c30a5e9b2f12b4e114f/faiss/faiss/python' does not exist\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install langchain\n",
        "!pip3 install wikipedia\n",
        "!pip3 install chromadb\n",
        "!pip3 install tiktoken\n",
        "!pip3 install pypdf\n",
        "!pip3 install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeaa6cWTeWOI",
        "outputId": "65e0b94c-7883-4bc2-ae94-45a82a64f670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /opt/homebrew/lib/python3.11/site-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1NCfM18ZRl8L"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wikipedia\n",
        "import urllib.request\n",
        "import bs4 as bs\n",
        "import os\n",
        "\n",
        "#import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# import Langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5WgfVqRRl8N"
      },
      "source": [
        "# Generative AI\n",
        "\n",
        "<img src='https://images.unsplash.com/photo-1686191568035-db49125b65c6?auto=format&fit=crop&q=80&w=2940&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D' width=\"450\">\n",
        "\n",
        "Credit: [Mojahid Mottakin](https://unsplash.com/@iammottakin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onO46LysT2dh"
      },
      "source": [
        "## Content\n",
        "\n",
        "The goal of this walkthrough is to provide you with insights on [Generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence). Generative AI refers to artificial intelligence systems that can create new and original content, such as text, images, or music, without direct human input.\n",
        "\n",
        "We will first see some applications that we can have with GenAI and the text. We will also see that using GenAI can be costly and we will finally try to fine tune a model.\n",
        "\n",
        "- [First steps with LangChain and OpenAI](#First-steps)\n",
        "    - [Claculate the cost](#Cost)\n",
        "- [Text Summarization](#Text-Summarization)\n",
        "- [Sentiment analysis](#Sentiment-analysis)\n",
        "- [Text embedding](#Text-embedding)\n",
        "- [Fine-tuning](#Fine-tuning)\n",
        "- [Your turn!](#exercices)\n",
        "    - [Prompt](#prompting)\n",
        "    - [Summarizing](#Summarizing)\n",
        "    - [Sentiment analysis](#Sentiment-analysis-ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv1T75j9YpS4"
      },
      "source": [
        "## First steps with LangChain and OpenAI\n",
        "During this exercise session, we will be using the [OpenAI](https://platform.openai.com/docs/api-reference) API with different libraries.\n",
        "First we will need to setup the apikey and some of the librairies.\n",
        "\n",
        "Due to recent changes in the openAI library, there are incompatibilities with the Langchain library. We will therfore use a previous version by using this command:\n",
        "\n",
        "```\n",
        "!pip install openai==0.28.1\n",
        "```\n",
        "We will also use the [Langchain](https://python.langchain.com/docs/get_started/introduction) library for the prompting and the text applications.\n",
        "\n",
        "First, we will define the API key. (This key usually has a price, but it has been given by prof. Vlachos for this week so that you can play with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNtXBIJGbB1Q",
        "outputId": "24e38e64-a8b8-4fdd-8ebf-c62fe65a73a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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\n"
          ]
        }
      ],
      "source": [
        "# define the openAI API key\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "print(os.getenv('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_A3akTUhHCH"
      },
      "source": [
        "To try the key, let's use ChatOpenAI like we would use ChatGPT. To do so, we will need to import the ChatOpenAI model:\n",
        "\n",
        "\n",
        "```\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aFDzxmgYsdD",
        "outputId": "ba4df36f-ed75-4224-ac15-5b19e36227b2"
      },
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "Incorrect API key provided: sk-sVEU8***************************************GY5Q. You can find your API key at https://platform.openai.com/account/api-keys.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/dimitriroulin/Documents/Master/Data Science & Machine Learning/Projet/Untitled/Week_09_Solutions.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dimitriroulin/Documents/Master/Data%20Science%20%26%20Machine%20Learning/Projet/Untitled/Week_09_Solutions.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m chat_model  \u001b[39m=\u001b[39m ChatOpenAI()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dimitriroulin/Documents/Master/Data%20Science%20%26%20Machine%20Learning/Projet/Untitled/Week_09_Solutions.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m llm \u001b[39m=\u001b[39m OpenAI()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dimitriroulin/Documents/Master/Data%20Science%20%26%20Machine%20Learning/Projet/Untitled/Week_09_Solutions.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(llm\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39;49m\u001b[39mHi! How are you?\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dimitriroulin/Documents/Master/Data%20Science%20%26%20Machine%20Learning/Projet/Untitled/Week_09_Solutions.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(chat_model\u001b[39m.\u001b[39mpredict(\u001b[39m\"\u001b[39m\u001b[39mHi! How are you?\u001b[39m\u001b[39m\"\u001b[39m))\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:916\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 916\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[1;32m    878\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    879\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    880\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    881\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    882\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[1;32m    884\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/openai.py:448\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    437\u001b[0m         {\n\u001b[1;32m    438\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         }\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    449\u001b[0m         \u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    450\u001b[0m     )\n\u001b[1;32m    451\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    452\u001b[0m         \u001b[39m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[1;32m    453\u001b[0m         \u001b[39m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m         response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdict()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/openai.py:120\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/openai.py:118\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-sVEU8***************************************GY5Q. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ],
      "source": [
        "# define the chat and try it with a simple message\n",
        "chat_model  = ChatOpenAI()\n",
        "llm = OpenAI()\n",
        "\n",
        "print(llm.predict(\"Hi! How are you?\"))\n",
        "print(chat_model.predict(\"Hi! How are you?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmORZnjrl15m"
      },
      "source": [
        "You can also use it differently as it is shown in the two following examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTeJiGm6iNtl",
        "outputId": "4b776444-ef8f-4015-b1f7-8f65ffeeedf5"
      },
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "Incorrect API key provided: sk-sVEU8***************************************GY5Q. You can find your API key at https://platform.openai.com/account/api-keys.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/dimitriroulin/Documents/Master/Data Science & Machine Learning/Projet/Untitled/Week_09_Solutions.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dimitriroulin/Documents/Master/Data%20Science%20%26%20Machine%20Learning/Projet/Untitled/Week_09_Solutions.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat would be a good startup name for startup from a student who just graduated from a degree in Information Systems and Digital Innovation?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dimitriroulin/Documents/Master/Data%20Science%20%26%20Machine%20Learning/Projet/Untitled/Week_09_Solutions.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(llm\u001b[39m.\u001b[39;49mpredict(text))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dimitriroulin/Documents/Master/Data%20Science%20%26%20Machine%20Learning/Projet/Untitled/Week_09_Solutions.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(chat_model\u001b[39m.\u001b[39mpredict(text))\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:916\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 916\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[1;32m    878\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    879\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    880\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    881\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    882\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[1;32m    884\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/openai.py:448\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    437\u001b[0m         {\n\u001b[1;32m    438\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         }\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    449\u001b[0m         \u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    450\u001b[0m     )\n\u001b[1;32m    451\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    452\u001b[0m         \u001b[39m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[1;32m    453\u001b[0m         \u001b[39m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m         response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdict()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/openai.py:120\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/llms/openai.py:118\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-sVEU8***************************************GY5Q. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ],
      "source": [
        "text = \"What would be a good startup name for startup from a student who just graduated from a degree in Information Systems and Digital Innovation?\"\n",
        "\n",
        "print(llm.predict(text))\n",
        "print(chat_model.predict(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXde7hOAiaVw",
        "outputId": "e2f7f03e-0ca4-404d-9125-c1a8dc2cf563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Answer: DigiThink Solutions.\n",
            "1. TechGenius\n",
            "2. InfoTech Innovators\n",
            "3. DigiSolutions\n",
            "4. InfoSys Ventures\n",
            "5. DigitalLeap\n",
            "6. CodeGenius\n",
            "7. TechInnovate\n",
            "8. InfoInnovators\n",
            "9. SystemX\n",
            "10. DigitalNexus\n",
            "11. InfoFusion\n",
            "12. TechEdge\n",
            "13. DigitalStratos\n",
            "14. InfoTech Catalyst\n",
            "15. CodeCrafters\n",
            "16. TechPioneers\n",
            "17. InfoSage\n",
            "18. DigitalSphere\n",
            "19. TechConnect\n",
            "20. InfoWave\n"
          ]
        }
      ],
      "source": [
        "text = \"What would be a good startup name for startup from a student who just graduated from a degree in Information Systems and Digital Innovation?\"\n",
        "messages = [HumanMessage(content=text)]\n",
        "\n",
        "print(llm.predict_messages(messages).content)\n",
        "print(chat_model.predict_messages(messages).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YqxMiZcmOxL"
      },
      "source": [
        "With Langchain, you can directly pass specifications for a prompt by using the ```PromptTemplate``` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8bxWPF9mkoyI",
        "outputId": "d02c9e1a-3923-4bac-b938-508e14adb498"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What is a good name for a startup that works in Information Systems and Digital Innovation?'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = PromptTemplate.from_template(\"What is a good name for a startup that works in {field}?\")\n",
        "prompt.format(field=\"Information Systems and Digital Innovation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti0RqVq8_Fzc"
      },
      "source": [
        "### Calculate the cost\n",
        "Everything has a cost and the chatGPT API is not free. It depends on how many tokens we are giving to the API. You can have the pricing [here](https://openai.com/pricing).\n",
        "\n",
        "To give you an idea of the costs, calculate the price of using the GPT-4 model with a document that contains 86'000 tokens and you want a summary of 1'000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPrLNLh69NNw",
        "outputId": "a0d42bbd-d2b8-4742-8a13-d7f82aadbb22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total cost is equal to 2.64$\n"
          ]
        }
      ],
      "source": [
        "# Your turn!\n",
        "tokens_input = 86000\n",
        "tokens_output = 1000\n",
        "cost_per_token_input = 0.00003\n",
        "cost_per_token_output = 0.00006\n",
        "total_cost = tokens_input*cost_per_token_input + tokens_output*cost_per_token_output\n",
        "print(f'The total cost is equal to {total_cost}$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pI-k5OMvn95"
      },
      "source": [
        "## Text Summarization\n",
        "GenerativeAI can be very useful to summarize data. We will continue to use [Langchain](https://python.langchain.com/docs/use_cases/summarization) for our implementation. We will try to summarize the Wikipedia page of [*Information System*](https://en.wikipedia.org/wiki/Information_system). We will first load the text, the model [gpt-3.5-turbo-16k](https://platform.openai.com/docs/models/gpt-3-5), which allow us to have 16k tokens, and the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyaUfkbNvpvp"
      },
      "outputs": [],
      "source": [
        "# load the text\n",
        "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Information_system\")\n",
        "docs = loader.load()\n",
        "\n",
        "# load the model and the chain\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k-0613\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl9w98ej_Nhj"
      },
      "source": [
        "### Calculate the cost\n",
        "Using openAI API is not free. You can calculate the cost of your model like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "yBCYdOpk_gIT",
        "outputId": "161f88ea-054d-4f62-b693-b0090b69ec9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens Used: 11431\n",
            "\tPrompt Tokens: 11316\n",
            "\tCompletion Tokens: 115\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.03440800000000001\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'An information system is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information. It consists of hardware, software, data, procedures, and people. Information systems can support business decisions, and they are also studied as an academic discipline. There are various types of information systems, such as transaction processing systems, decision support systems, and enterprise systems. Information systems research focuses on the effects of information systems on individuals, groups, and organizations. Careers in information systems include information system strategy, management information systems, project management, and more.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#get the costs\n",
        "with get_openai_callback() as cb:\n",
        "  result = chain.run(docs)\n",
        "  print(cb)\n",
        "  display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsQoyYkH1lwM"
      },
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "By using Langchain, we can also classify the text. You can give different categories to the model and it will try to classify the sentence. For example, we will do a sentiment analysis and we will try to recognize in which language it is written.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI6x1BhRJPHb"
      },
      "outputs": [],
      "source": [
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\":[\"positive\", \"neutral\", \"negative\"],\n",
        "        },\n",
        "        \"language\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"spanish\", \"english\", \"french\", \"german\", \"italian\"],\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"sentiment\", \"language\"],\n",
        "}\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
        "chain = create_tagging_chain(schema, llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlb1g9BWO5KS",
        "outputId": "10d5fa76-4aa5-4f61-ed84-640992691b27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentiment': 'positive', 'language': 'english'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test\n",
        "input = \"I love dogs!\"\n",
        "\n",
        "chain.run(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_SUKEgOSnOD"
      },
      "source": [
        "## Text embedding\n",
        "LangChain is also useful for text embedding. You can see the different models [here](https://python.langchain.com/docs/integrations/text_embedding/). You can have a look at the natural language processing class if you don't remeber why text embedding is important.\n",
        "\n",
        "You can either do the text embedding for a list of texts or a single piece of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AwioZxzZz4W"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55i2zcA2YW3s",
        "outputId": "49878615-a225-46cd-fd59-d688c204af3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.003105442842703499,\n",
              " 0.011136301333111021,\n",
              " -0.004029582553785511,\n",
              " -0.01174906406444312,\n",
              " -0.0010406979861090607]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#embeding a single query\n",
        "text = \"This is a test document.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "query_result[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w89IGJ2IZ2eo",
        "outputId": "174620cf-0996-4008-d607-97684c8182d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 1536)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#embeding a list of texts\n",
        "embedds = embeddings.embed_documents(\n",
        "    [\n",
        "        \"Hi there!\",\n",
        "        \"Oh, hello!\",\n",
        "        \"What's your name?\",\n",
        "        \"My friends call me World\",\n",
        "        \"Hello World!\"\n",
        "    ]\n",
        ")\n",
        "len(embedds), len(embedds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ-TviJxmMsS"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Fine-tuning refers to the process of taking a pre-trained model (like chatGPT) and further training it on a specific task or dataset to improve its performance on that particular task. Instead of training a model from scratch, which can be computationally expensive and time-consuming, fine-tuning leverages the knowledge and features learned by a model on a large and diverse dataset.\n",
        "\n",
        "You can have a more detailed article [here](https://medium.com/@dataoilst.info/fine-tuning-langchain-llm-applications-a-technical-perspective-part-1-4b4c552ab557).\n",
        "\n",
        "For now, we will use the previous text and try to ask the model a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F82FIQoZatwK",
        "outputId": "69095d99-a995-4a0e-d20f-23427f21f08a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 3652, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1389, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2112, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2441, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1599, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1921, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2605, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2020, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3724, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1616, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3163, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1153, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "#processing the document\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZimOCk4a9dD",
        "outputId": "1ce62203-b84a-457b-a770-f89fd9107b08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System that supports business decisions\n",
            "\"Business Information System\" redirects here. For the Finnish government service, see YTJ (Finnish government service).\n",
            "An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information.[1] From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology.[2] Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.[3]\n",
            "Bachelor of Business Information Systems  BBIS[4]  is an Information Technology(IT) and management focused undergraduate program designed to better understand the needs of rapidly growing technology in business and IT sector.It is bachelor degree that combines elements of business administration and computer science with majoring on information systems and technology.The purpose of this course is to equip students with the skills and knowledge needed to effectively manage and utilize information technology in a business and IT industry.\n",
            "A computer information system is a system that is composed of people and computers that processes or interprets information.[5][6][7][8] The term is also sometimes used to simply refer to a computer system with software installed.\n",
            "\"Information systems\" is also an academic field study about systems with a specific reference to information and the complementary networks of computer hardware and software that people and organizations use to collect, filter, process, create and also distribute data.[9] An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.[10]\n",
            "In many organizations, the department or unit responsible for information systems and data processing is known as \"information services\".[11][12][13][14]\n",
            "Any specific information system aims to support operations, management and decision-making.[15][16] An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.[17]\n",
            "Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end-use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.[18]\n",
            "Alter[19][20] argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information.[21]\n",
            "As such, information systems inter-relate with data systems on the one hand and activity systems on the other.[22] An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action.\n",
            "Information systems are the primary focus of study for organizational informatics.[23]\n"
          ]
        }
      ],
      "source": [
        "#asking the question\n",
        "retrieved_docs = retriever.invoke(\n",
        "    \"What is an information system?\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU4dqJmMeRYO"
      },
      "source": [
        "We can see that the model is simply giving us the parts where information system is mentionned. Therfore, the model is not very useful. You can however go a bit further if you wish to integrate it better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owDlwBaLXp0"
      },
      "source": [
        "## Your turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbF60XZevym"
      },
      "source": [
        "### Prompt\n",
        "Create a prompt to ask chatGPT some project ideas for a Data Science and Machine Learning class. Try to apply some rules of [prompt engineering](https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFZ5t75WfMXr",
        "outputId": "be88d514-d254-4639-b67c-cc21fa6e400b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Predictive maintenance: Develop a machine learning model to predict maintenance requirements for industrial equipment, helping organizations minimize downtime and save costs.\n",
            "2. Fraud detection: Build an anomaly detection system using machine learning algorithms to identify fraudulent transactions in real-time, assisting financial institutions in preventing potential losses.\n",
            "3. Recommender system: Create a personalized recommendation engine for an e-commerce platform, leveraging collaborative filtering techniques to suggest products based on user preferences.\n",
            "4. Sentiment analysis: Develop a sentiment analysis tool that analyzes customer reviews or social media data to determine public opinion about a product or service, assisting companies in understanding customer sentiment.\n",
            "5. Traffic prediction: Build a machine learning model that predicts traffic congestion patterns based on historical data, helping commuters plan their routes more efficiently and reduce travel time.\n",
            "6. Disease prediction: Develop a predictive model that analyzes patient data to detect early signs of diseases such as diabetes or heart conditions, aiding in proactive healthcare interventions.\n",
            "7. Stock market prediction: Create a machine learning model that predicts stock market trends based on historical data and market indicators, assisting investors in making informed decisions.\n",
            "8. Image recognition: Build a deep learning model that can accurately classify and recognize objects in images, enabling applications such as autonomous vehicles or facial recognition systems.\n",
            "9. Natural language processing: Develop a language understanding system using NLP techniques to perform tasks like sentiment analysis, text summarization, or chatbot development, enhancing customer support or content analysis.\n",
            "10. Energy consumption optimization: Build a predictive model that analyzes energy usage patterns and identifies areas for optimization in buildings or industrial processes, helping organizations reduce energy consumption and environmental impact.\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "chat_model  = ChatOpenAI()\n",
        "\n",
        "text = \"As a student in a class of Data Science and Machine learning that is part of a Master in Information Systems and Digital Innovation, list 10 project innovative ideas to do in Python\"\n",
        "\n",
        "print(chat_model.predict(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAG2jjy_wGhh"
      },
      "source": [
        "### Summarizing\n",
        "\n",
        "You will now try to summarize one of your lesson and print how much it costed by using the model ```gpt-3.5-turbo-16k-0613```. You might want to look at the package ```PyPDFLoader``` in order to load your pdf.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAQQxguTyI9i"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "pdf_loader = PyPDFLoader(\"IoT.pdf\")\n",
        "pages = pdf_loader.load_and_split()\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k-0613\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "uu_5TJS2zFxC",
        "outputId": "06482d62-233e-49d3-8787-07a4f85d24a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens Used: 2006\n",
            "\tPrompt Tokens: 1818\n",
            "\tCompletion Tokens: 188\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.006206000000000001\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This article discusses the concept of the Internet of Things (IoT) and its applications. It explains that IoT involves physical objects embedded with sensors, software, and other technologies that connect and exchange data with other devices over the internet. The article also mentions related terms such as edge computing and Industry 4.0. It compares different devices used in IoT projects, such as Arduino, Raspberry Pi, and ESP32. The article provides information on the features and specifications of the ESP32 device that will be used in the hands-on part of the article. It also mentions various learning resources and project ideas for IoT. The hands-on part of the article includes tutorials on using UIFlow to create interfaces and perform various tasks with the device. It also discusses projects involving reading data from the internet and sensors, writing data to the internet, and controlling the device from the internet. The article concludes with a list of selected lessons and terminology related to IoT.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with get_openai_callback() as cb:\n",
        "  result = chain.run(pages)\n",
        "  print(cb)\n",
        "  display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPPhytTFRQNz"
      },
      "source": [
        "### Sentiment analysis\n",
        "Try to classify this sentence: ```J'aime l'intelligence artificielle.```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OpKnsJ1BRhR7",
        "outputId": "7ad079fa-41cb-4476-8033-f3e07e0a5575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentiment': 'positive', 'language': 'french'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Your code\n",
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sentiment\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\":[\"positive\", \"neutral\", \"negative\"],\n",
        "        },\n",
        "        \"language\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"spanish\", \"english\", \"french\", \"german\", \"italian\"],\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"sentiment\", \"language\"],\n",
        "}\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
        "chain = create_tagging_chain(schema, llm)\n",
        "input = \"J'aime l'intelligence artificielle.\"\n",
        "display(chain.run(input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLdHH2FvRiYj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
