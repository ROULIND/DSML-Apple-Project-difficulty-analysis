{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: spacy in /opt/homebrew/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from spacy) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install spacy\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GET THE LIBRAIRIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from tensorflow.keras import preprocessing as kprocessing\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "import transformers\n",
    "\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Various\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GET THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence difficulty\n",
       "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
       "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
       "2   2  Le test de niveau en français est sur le site ...         A1\n",
       "3   3           Est-ce que ton mari est aussi de Boston?         A1\n",
       "4   4  Dans les écoles de commerce, dans les couloirs...         B1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the data from the given URLs and store them in dataframes\n",
    "sample_submission_df = pd.read_csv('sample_submission.csv')\n",
    "training_data_df = pd.read_csv('training_data.csv')\n",
    "unlabelled_test_data_df = pd.read_csv('unlabelled_test_data.csv')\n",
    "\n",
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news: 4800\n",
      "----------------------------------------\n",
      "Split by category:\n",
      "difficulty\n",
      "A1    813\n",
      "C2    807\n",
      "C1    798\n",
      "B1    795\n",
      "A2    795\n",
      "B2    792\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Number of categories: 6\n"
     ]
    }
   ],
   "source": [
    "print('Total number of news: {}'.format(len(training_data_df)))\n",
    "print(40*'-')\n",
    "print('Split by category:')\n",
    "print(training_data_df[\"difficulty\"].value_counts())\n",
    "print(40*'-')\n",
    "nr_categories = len(training_data_df[\"difficulty\"].unique())\n",
    "print(\"Number of categories: {n}\".format(n=nr_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  B2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text:\n",
      "Que vous cherchiez une simple annonce ou un recruteur, vous êtes sûr de ne rater aucune offre d'emploi grâce à notre système d'alerte personnalisé.\n"
     ]
    }
   ],
   "source": [
    "# You can adjust n:\n",
    "n=100\n",
    "print('Category: ',training_data_df[\"difficulty\"][n])\n",
    "print(100*'-')\n",
    "print('Text:')\n",
    "print(training_data_df['sentence'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dimitriroulin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "      <td>coût kilométrique réel pouvoir diverger sensib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "      <td>bleu cest couleur préférer naim vert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "      <td>test niveau français site internet lécole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "      <td>estce mari aussi boston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "      <td>école commerc couloir place financier arriver ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence difficulty  \\\n",
       "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
       "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
       "2   2  Le test de niveau en français est sur le site ...         A1   \n",
       "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
       "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
       "\n",
       "                                          text_clean  \n",
       "0  coût kilométrique réel pouvoir diverger sensib...  \n",
       "1               bleu cest couleur préférer naim vert  \n",
       "2          test niveau français site internet lécole  \n",
       "3                            estce mari aussi boston  \n",
       "4  école commerc couloir place financier arriver ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# Download French stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"french\")\n",
    "\n",
    "# Load French tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    # Clean text\n",
    "    text = str(text).lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    lst_text = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "\n",
    "    # Lemmatization with spaCy (if flag is true)\n",
    "    if flg_lemm == True:\n",
    "        lst_text = [token.lemma_ for token in nlp(\" \".join(lst_text))]\n",
    "\n",
    "    # Back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "training_data_df[\"text_clean\"] = training_data_df[\"sentence\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "\n",
    "# Display the DataFrame\n",
    "training_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming, Input -> X, Output -> y\n",
    "X = training_data_df['text_clean']\n",
    "y = training_data_df['difficulty']\n",
    "# Split into Training and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=training_data_df['difficulty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=15000, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=15000, ngram_range=(1, 2))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=15000, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = X_train\n",
    "# Initizalize the vectorizer with max nr words and ngrams (1: single words, 2: two words in a row)\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1,2))\n",
    "# Fit the vectorizer to the training data\n",
    "vectorizer_tfidf.fit(corpus)\n",
    "TfidfVectorizer(max_features=15000, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_tfidf = LogisticRegression()\n",
    "model_tfidf = Pipeline([(\"vectorizer\", vectorizer_tfidf), (\"classifier\", classifier_tfidf)])\n",
    "\n",
    "start_time = datetime.now()\n",
    "model_tfidf.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "training_time_tfidf = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training data: 95.1%\n",
      "Accuracy Test data: 40.8%\n",
      "Training time: 0.6s\n"
     ]
    }
   ],
   "source": [
    "predicted_train_tfidf = model_tfidf.predict(X_train)\n",
    "accuracy_train_tfidf = accuracy_score(y_train, predicted_train_tfidf)\n",
    "print('Accuracy Training data: {:.1%}'.format(accuracy_train_tfidf))\n",
    "\n",
    "predicted_test_tfidf = model_tfidf.predict(X_test)\n",
    "accuracy_test_tfidf = accuracy_score(y_test, predicted_test_tfidf)\n",
    "accuracy_tfidf = accuracy_test_tfidf\n",
    "print('Accuracy Test data: {:.1%}'.format(accuracy_test_tfidf))\n",
    "\n",
    "print('Training time: {:.1f}s'.format(training_time_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes of the model:  ['A1' 'A2' 'B1' 'B2' 'C1' 'C2']\n",
      "--------------------------------------------------------------------------------\n",
      "Shape of the coefficients of the model (categories x vocabulary size):  (6, 15000)\n",
      "--------------------------------------------------------------------------------\n",
      "A1: mappelle, jaim, mange, estce, jaime, aller, cest, vai, bonjour, merci, \n",
      "--------------------------------------------------------------------------------\n",
      "A2: avant, serveur, fai, venir, lécole, faire, dire, aller, jai, cest, \n",
      "--------------------------------------------------------------------------------\n",
      "B1: trop, france, quoi, ville, exemple, faire, prince, savoir, avoir, quil, \n",
      "--------------------------------------------------------------------------------\n",
      "B2: communication, vers, journaliste, rendre, chaussure, capacité, électronique, forêt, portable, déchet, \n",
      "--------------------------------------------------------------------------------\n",
      "C1: édition, fabrice, vie, an, population, 000, fort, bison, leur, professionnel, \n",
      "--------------------------------------------------------------------------------\n",
      "C2: sou, sans, regard, pouvoir, liberté, dont, langue, dun, ce, comme, \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Classes of the model: ',classifier_tfidf.classes_)\n",
    "print(80*'-')\n",
    "print('Shape of the coefficients of the model (categories x vocabulary size): ',classifier_tfidf.coef_.shape)\n",
    "print(80*'-')\n",
    "NN = 10\n",
    "# Get the 10 (here: NN, which you can adjust yourself) ids of the words with highest weights per category\n",
    "top_words = np.argsort(classifier_tfidf.coef_,axis=1)[:,-NN:]\n",
    "\n",
    "# Get the vocabulary of the model (mapping of words to ids):\n",
    "voc = vectorizer_tfidf.vocabulary_\n",
    "# Get the inverse vocabulary to map the ids of the words to the words:\n",
    "inv_voc = {v: k for k, v in voc.items()}\n",
    "\n",
    "# Get for each category (=class) the top ten words\n",
    "for n, w in enumerate(classifier_tfidf.classes_):\n",
    "    t = w + ': '\n",
    "    for i in range(NN):\n",
    "        t += inv_voc[top_words[n,i]]\n",
    "        if i!=NN:\n",
    "            t+=', '\n",
    "    print(t)\n",
    "    print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dimitriroulin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "Accuracy of the model: 43.7%\n",
      "\ttfidf__max_df: 0.5\n",
      "Accuracy of the model: 43.7%\n",
      "\ttfidf__max_features: None\n",
      "Accuracy of the model: 43.7%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Get French stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "french_stopwords = stopwords.words('french')\n",
    "\n",
    "# Define a pipeline combining a text feature extractor with a simple classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=french_stopwords)),  # Use the custom stopwords list\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Define the parameter space for grid search\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__max_features': (None, 5000, 10000),\n",
    "    'clf__alpha': (0.1, 1, 10),\n",
    "}\n",
    "\n",
    "# Perform grid search on the classifier using the defined parameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    # Get the best model from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict the labels of the test set using the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of the model: {:.1%}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best parameters set:\n",
      "\tclf__C: 10\n",
      "\tclf__kernel: 'rbf'\n",
      "\ttfidf__max_df: 0.25\n",
      "\ttfidf__max_features: None\n",
      "Accuracy on the training set: 96.0%\n",
      "Accuracy on the test set: 43.7%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define a pipeline combining a text feature extractor with a simple classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=french_stopwords)),  # Use the custom stopwords list\n",
    "    ('clf', SVC()),  # Use SVC instead of MultinomialNB\n",
    "])\n",
    "\n",
    "# Define the parameter space for grid search\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__max_features': (None, 5000, 10000, 20000),\n",
    "    'clf__C': (0.1, 1, 10),  # Add parameters for SVC\n",
    "    'clf__kernel': ('linear', 'rbf'),  # Add parameters for SVC\n",
    "}\n",
    "\n",
    "# Perform grid search on the classifier using the defined parameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    " # Calculate the accuracy of the model on the training set\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Accuracy on the training set: {:.1%}\".format(accuracy_train))\n",
    "print(\"Accuracy on the test set: {:.1%}\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding of labels into a one-hot vector: B2 is  [0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Renaming, Input -> X, Output -> y\n",
    "X_tf = training_data_df['sentence']\n",
    "y_tf_class = training_data_df['difficulty']\n",
    "\n",
    "# Convert labels into a one-hot vector of size 5 (the number of distinct labels)\n",
    "lab = LabelBinarizer()\n",
    "lab.fit(y_tf_class)\n",
    "y_tf = lab.transform(y_tf_class)\n",
    "\n",
    "# Example (you can modify n)\n",
    "n=100\n",
    "print('Coding of labels into a one-hot vector: ' + y_tf_class[n] + ' is ', y_tf[n])\n",
    "\n",
    "# Split into training and test data\n",
    "X_tf_train, X_tf_test, y_tf_train, y_tf_test = train_test_split(X_tf, y_tf, test_size=0.3, random_state=42, stratify=training_data_df['difficulty'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = X_tf_train\n",
    "max_words = 15000\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', num_words=max_words, oov_token=\"<pad>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "voc = tokenizer.word_index\n",
    "reverse_voc = dict([(value, key) for (key, value) in voc.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (3360, 200)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0   15    4  161  213 4932 4933 4934\n",
      " 4935    5 4936 4937]\n"
     ]
    }
   ],
   "source": [
    "max_len = 200\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_tf_train)\n",
    "X_tf_train_seq = kprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Apply the same to test data\n",
    "X_tf_test_seq = kprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_tf_test), maxlen=max_len)\n",
    "\n",
    "n=10 # You can adjust n\n",
    "print('Shape: ',X_tf_train_seq.shape)\n",
    "print(100*'-')\n",
    "print('Example: ',X_tf_train_seq[n,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
