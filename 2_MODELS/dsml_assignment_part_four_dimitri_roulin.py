# -*- coding: utf-8 -*-
"""DSML - Assignment_part_four-Dimitri-Roulin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yXuO5nnX21G4GTl2BBcB2xuKGd6r9SM2

<a href="https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%204/Assignment_part_four.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

DSML investigation:

You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.

Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!

# Assignement part four
### Due 22.10
#### Identifying the suspects credit score
We received informations that the rogue agent has a good credit score.

Our spies at SIMF have managed to collect financial information relating to our suspects as well as a training dataset.

Create a Neural Network over the training dataset `df` to identify which of the suspects have a good Credit_Mix


## Getting to know our data

* Age: a users age
* Occupation: a users employment field
* Annual_Income: a users annual income
* Monthly_Inh_Salary: the calculated salary received by a given user on a monthly basis
* Num_Bank_Accounts: the number of bank accounts possessed by a given user
* Num_Credit_Cards: the number of credit card given user possesses
* Interest_Rate: The interest rate on those cards (if multiple then its the average)
* Num_of_Loans: The number of loans of each user
* Delay_from_due_date: payment tardiness of user
* Num_of_Delayed_Payment: the count of delayed payments
* Changed_Credit_Limit: NaN
* Num_Credit_Inquiries: NaN
* Credit_Mix: The users credit score
* Outsting_Debt: Outstanding debt
* Credit_Utilization_Ratio: the percentage of borrowed money over borrowing allowance
* Payment_of_Min_Amount: does the user usually pay the minimal amount (categorical)
* Total_EMI_per_month: Monthly repayments to be made
* Amount_invested_monthly: The amout put in an investment fun by the user on a monthly basis
* Payment_Behaviour: the users payment behavior (categorical)
* Monthly_Balance: The users end of the month balance
* AutoLoan: If the user has an active loan for their vehicule
* Credit-BuilderLoan: If the user has a loan to increase their credit score
* DebtConsolidationLoan, HomeEquityLoan, MortgageLoan, NotSpecified, PaydayLoan, PersonalLoan, StudentLoan: different types of loans(categorical features)
"""

# Commented out IPython magic to ensure Python compatibility.
# Import required packages
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler

# %matplotlib inline

df = pd.read_csv("https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/train_classification.csv", index_col='Unnamed: 0').dropna()
suspects = pd.read_csv("https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/suspects.csv", index_col='Unnamed: 0').dropna()

print(df.shape)
df.head()

df["Credit_Mix"].value_counts()

"""# 1. Preparing the data
## 1.1 Data cleaning
 Perform OHE over the "Occupation" feature

 Then, perform LE over Payment_of_Min_Amount and Payment_Behaviour

 _hint: As we will be testing only one model no need to define a pipeline_
"""

# Apply OneHotEncoding over the Occupation feature
df_train = df
dummies_occupation = pd.get_dummies(df_train['Occupation'], prefix='Occupation')
df_train = pd.concat([df_train, dummies_occupation], axis=1)
df_train.drop('Occupation', axis=1, inplace=True)

# Label Encoder over Payment_of_Min_Amount & Payment_Behaviour
label_enc_min_amount = LabelEncoder()
df_train['Payment_of_Min_Amount'] = label_enc_min_amount.fit_transform(df_train['Payment_of_Min_Amount'])

label_enc_pay_behaviour = LabelEncoder()
df_train['Payment_Behaviour'] = label_enc_pay_behaviour.fit_transform(df_train['Payment_Behaviour'])

df_train.head()

"""## 1.2 Dataset splitting

Split the dataset in two, first X with your independent features and then y with the dependent feature **CreditMix**.

Then perform :
* OneHotEncoding over the **CreditMix** feature.
* A MinMaxScaller over the independent features
"""

# Your code here:
X = df_train.drop(columns=["Credit_Mix"])

y_ohe = pd.get_dummies(df_train['Credit_Mix'], prefix='Credit_Mix')



#Define the scaler
scaler = MinMaxScaler()
#Fit the scaler
scaler.fit(X)
X_scaled = pd.DataFrame(scaler.transform(X), columns=X.columns, index=X.index)

"""### 1.2.1 Train Test splitting
Now split the data in X_train, X_test, y_train, y_test,

You can use test_size = 0.2 and a random_state of 42
"""

# Your code here
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_ohe, test_size=0.2, random_state=42)

"""### 1.2.2 final touches
Convert your datasets to `Torch tensors` of type `torch.float`
"""

#Your code here:
# Transform the data into tensor
X_train_sensor = torch.tensor(X_train.values, dtype=torch.float)
y_train_sensor = torch.tensor(y_train.values, dtype=torch.float)
X_test_sensor = torch.tensor(X_test.values, dtype=torch.float)
y_test_sensor = torch.tensor(y_test.values, dtype=torch.float)

print(X_train_sensor.size(), y_train_sensor.size())

"""# 2 Model preparation:

## 2.1 Define a Neural network model and instantiate it.
You can set the number of neurons to 150.
"""

# Define a neural network class here:
class Net_lab(nn.Module):
    def __init__(self, D_in, H1, D_out):
        super(Net_lab, self).__init__()

        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for first hidden layer
        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer
        self.activation = nn.ReLU()               # Activation function for hidden layer

    def forward(self, x):
        y_pred = self.activation(self.linear1(x))        # Hidden layer: linear transformation + ReLU
        y_pred = self.linear2(y_pred)                    # Output layer: linear transformation
        return y_pred

# Instantiate your model here
D_in, D_out = X_train_sensor.shape[1], y_train_sensor.shape[1]
print(D_in, D_out)
model_lab = Net_lab(D_in, 150, D_out)

# calculate how many parameters the model has
params = sum(p.numel() for p in model_lab.parameters() if p.requires_grad)
display(params) # (42*150) + 150 + (150*3) + 3 = 6903

"""## 2.2 finding the best model:
Identify, amongst the following options the best parameters for your model:

* `criterion` : [CrossEntropyLoss](https://anvilproject.org/guides/content/creating-links), [BCEWithLogitsLoss](hhttps://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)
* `iterations` : 150, 250, 500
* `learning rate` : 0.00005, 0.001, 12.031


_Hint: restart your runtime between each execution to ensure that previous neural networks dont interfere with your current one_

You can evaluate your model based on it's accuracy over the test set
"""

# Define your loss function here:
losses_lab = []
losses_test_lab = []

criterion_CEL = nn.CrossEntropyLoss()
criterion_BCE = nn.BCEWithLogitsLoss()


# Define your Adam optimizer for finding the weights of the network here
optimizer1 = torch.optim.Adam(model_lab.parameters(), lr=0.00005)
optimizer2 = torch.optim.Adam(model_lab.parameters(), lr=0.001)
optimizer3 = torch.optim.Adam(model_lab.parameters(), lr=12.031)

def getAccuracy(X_sensor, y_sensor, model):
    #Test accuracy for each combinaison of params
    y_pred = model(X_sensor)
    # Trouver l'indice de la valeur maximale dans chaque ligne
    _, indices = torch.max(y_pred, dim=1)

    # Créer un nouveau tensor en remplaçant la valeur maximale par 1 et les autres par 0
    y_pred_enc = torch.zeros_like(y_pred)
    y_pred_enc[torch.arange(y_pred.size(0)), indices] = 1

    correct = (y_sensor == y_pred_enc).all(dim=1).sum().item()
    total = len(y_sensor)

    accuracy = correct/total

    return accuracy

def getInfos(criterion, learning_rate, losses_list ,accuracies_list):
  print(f'criterion={criterion}, lr={learning_rate}, iteration=150    :    loss={losses_list[0]} & train_accuracy: {accuracies_list[0][0]}  & test_accuracy: {accuracies_list[0][1]} ')
  print(f'criterion={criterion}, lr={learning_rate}, iteration=250    :    loss={losses_list[1]} & train_accuracy: {accuracies_list[1][0]}  & test_accuracy: {accuracies_list[1][1]} ')
  print(f'criterion={criterion}, lr={learning_rate}, iteration=500    :    loss={losses_list[2]} & train_accuracy: {accuracies_list[2][0]}  & test_accuracy: {accuracies_list[2][1]} ')

# Perform your iterations here
accuracies_list = []
losses_list = []

for t in range(500):

    # Forward pass: compute prediction on training set
    y_pred = model_lab(X_train_sensor)

    # Compute loss
    loss = criterion_CEL(y_pred, y_train_sensor)
    print(t, loss.item())

    losses_lab.append(loss.item())
    if torch.isnan(loss):
        break

    # Compute gradient
    optimizer2.zero_grad()
    loss.backward()

    # Update
    optimizer2.step()

    # Compute loss on test set
    losses_test_lab.append(criterion_CEL(model_lab(X_test_sensor), y_test_sensor).item())

    if(t == 149) or (t == 249) or (t == 499):
      accuracies_list.append([getAccuracy(X_train_sensor, y_train_sensor, model_lab), getAccuracy(X_test_sensor, y_test_sensor, model_lab)])
      losses_list.append(loss.item())

getInfos('CEL', '0.001', losses_list, accuracies_list )

    # ------- Résultats -------- #
    # criterion=CEL, lr=0.00005, iteration=150    :    loss=1.1034873723983765 & train_accuracy: 0.2950637351355976  & test_accuracy: 0.29820359281437125
    # criterion=CEL, lr=0.00005, iteration=250    :    loss=1.1027215719223022 & train_accuracy: 0.29609034134656514  & test_accuracy: 0.2992301112061591
    # criterion=CEL, lr=0.00005, iteration=500    :    loss=1.1008518934249878 & train_accuracy: 0.30071006929591926  & test_accuracy: 0.30624465355004277

    # criterion=CEL, lr=0.001, iteration=150    :    loss=0.4813656508922577 & train_accuracy: 0.778723586277697  & test_accuracy: 0.7712574850299401
    # criterion=CEL, lr=0.001, iteration=250    :    loss=0.41514360904693604 & train_accuracy: 0.815168106767046  & test_accuracy: 0.8068434559452523
    # criterion=CEL, lr=0.001, iteration=500    :    loss=0.35103195905685425 & train_accuracy: 0.8431431260159123  & test_accuracy: 0.8378100940975193

    # criterion=CEL, lr=12.031, iteration=150    :    loss=3.5623302459716797 & train_accuracy: 0.460432885618958  & test_accuracy: 0.4545765611633875
    # criterion=CEL, lr=12.031, iteration=250    :    loss=3.5943896770477295 & train_accuracy: 0.460432885618958  & test_accuracy: 0.4545765611633875
    # criterion=CEL, lr=12.031, iteration=500    :    loss=2.7356956005096436 & train_accuracy: 0.460432885618958  & test_accuracy: 0.4545765611633875

    # criterion=BCE, lr=0.00005, iteration=150    :    loss=0.7128372192382812 & train_accuracy: 0.2433056719993156  & test_accuracy: 0.23849443969204448
    # criterion=BCE, lr=0.00005, iteration=250    :    loss=0.7125115990638733 & train_accuracy: 0.24369064932842843  & test_accuracy: 0.2390076988879384
    # criterion=BCE, lr=0.00005, iteration=500    :    loss=0.7117025256156921 & train_accuracy: 0.24471725553939602  & test_accuracy: 0.24020530367835757

    # criterion=BCE, lr=0.001, iteration=150    :    loss=0.6858038902282715 & train_accuracy: 0.4598768072546839  & test_accuracy: 0.4547476475620188
    # criterion=BCE, lr=0.001, iteration=250    :    loss=0.682219386100769 & train_accuracy: 0.4599623577722645  & test_accuracy: 0.4545765611633875
    # criterion=BCE, lr=0.001, iteration=500    :    loss=0.6740065217018127 & train_accuracy: 0.460432885618958  & test_accuracy: 0.4545765611633875

    # criterion=BCE, lr=12.031, iteration=150    :    loss=0.5605394840240479 & train_accuracy: 0.5804174865257935  & test_accuracy: 0.5705731394354149
    # criterion=BCE, lr=12.031, iteration=250    :    loss=0.6164559721946716 & train_accuracy: 0.460432885618958  & test_accuracy: 0.4545765611633875
    # criterion=BCE, lr=12.031, iteration=500    :    loss=0.6165924072265625 & train_accuracy: 0.460432885618958  & test_accuracy: 0.4545765611633875

#Test accuracy for each combinaison of params
y_pred = model_lab(X_train_sensor)
print(y_pred)

# Trouver l'indice de la valeur maximale dans chaque ligne
_, indices = torch.max(y_pred, dim=1)

print(indices)

# Créer un nouveau tensor en remplaçant la valeur maximale par 1 et les autres par 0
y_pred_enc = torch.zeros_like(y_pred)
print(y_pred_enc)
y_pred_enc[torch.arange(y_pred.size(0)), indices] = 1
print(y_pred_enc)

correct = (y_train_sensor == y_pred_enc).all(dim=1).sum().item()
total = len(y_train_sensor)

accuracy = correct/total

"""## 2.3 Model Accuracy
Identify the models accuracy over the train and test parts of the training dataset
"""

# deactivate dropout layers

# Training accuracy


# Test accuracy

"""# 3. Predictions over the suspects dataset
## 3.1 Retrain a new model over the full training dataset
#### Please use the following parameters for this section:
* ``neurons`` = 150
* ``learning`` rate = 0.00005
* ``criterion`` = CrossEntropyLoss
* `iterations` = 500

_hint you may have to redo some preprocessing as you did in part one_
"""

# Define a new model here:
X = df_train.drop(columns=["Credit_Mix"])

y_ohe = pd.get_dummies(df_train['Credit_Mix'], prefix='Credit_Mix')

X_suspects = suspects.drop(columns=["userID"])[X.columns.tolist()]


full_scaler = MinMaxScaler()

scaler.fit(X)
X_full_scaled = pd.DataFrame(scaler.transform(X), columns=X.columns, index=X.index)
X_suspects_scaled = pd.DataFrame(scaler.transform(X_suspects), columns=X_suspects.columns, index=X_suspects.index)
X_suspects_scaled.head()

y_ohe

X_full_sensor = torch.tensor(X_full_scaled.values, dtype=torch.float)
y_full_sensor = torch.tensor(y_ohe.values, dtype=torch.float)
X_suspects_sensor = torch.tensor(X_suspects_scaled.values, dtype=torch.float)

# Instantiate your model here
D_in, D_out = X_full_sensor.shape[1], y_full_sensor.shape[1]
print(D_in, D_out)
model_full = Net_lab(D_in, 150, D_out)

# calculate how many parameters the model has
params = sum(p.numel() for p in model_full.parameters() if p.requires_grad)
display(params)

# Define your MSE loss here:
losses_lab = []
losses_test_lab = []

# perform your training here
accuracies_list = []
losses_list = []

optimizer1 = torch.optim.Adam(model_full.parameters(), lr=0.00005)

for t in range(500):

    # Forward pass: compute prediction on training set
    y_pred = model_full(X_full_sensor)

    # Compute loss
    loss = criterion_CEL(y_pred, y_full_sensor)
    print(t, loss.item())

    losses_lab.append(loss.item())
    if torch.isnan(loss):
        break

    # Compute gradient
    optimizer1.zero_grad()
    loss.backward()

    # Update
    optimizer1.step()



    #if(t == 149) or (t == 249) or (t == 499):
    #  accuracies_list.append([getAccuracy(X_full_sensor, y_full_sensor, model_full), getAccuracy(X_full_sensor, y_full_sensor, model_full)])
    #  losses_list.append(loss.item())

#getInfos('CEL', '0.00005', losses_list, accuracies_list )

"""## 3.2 Predict over the suspects dataset"""

# Predict which users have a good credit score here:

y_suspects_predict = model_full(X_suspects_sensor)

# Trouver l'indice de la valeur maximale dans chaque ligne
_, indices = torch.max(y_suspects_predict, dim=1)

# Créer un nouveau tensor en remplaçant la valeur maximale par 1 et les autres par 0
y_suspects_predict_enc = torch.zeros_like(y_suspects_predict)


y_suspects_predict_enc[torch.arange(y_suspects_predict.size(0)), indices] = 1



column_names = ['Credit_Mix_Bad', 'Credit_Mix_Good', 'Credit_Mix_Standard']
df_predict = pd.DataFrame(y_suspects_predict_enc.numpy(), columns=column_names)


df_suspects_pred = pd.concat([suspects.reset_index(), df_predict], axis=1)


df_suspects_pred_goodonly = df_suspects_pred.loc[df_suspects_pred['Credit_Mix_Bad'] == 1]

df_suspects_pred_goodonly[['userID', 'Credit_Mix_Bad']]